{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_0 = pd.read_excel(r'C:\\Users\\Riddhima\\Downloads\\marketing_campaign.xlsx')\n",
    "df=df_0.copy()\n",
    "#df.head()\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df['customer_lifetime'] = 2024 - df['Dt_Customer'].dt.year\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Dt_Customer'],axis=1)  # we drop the column \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df.isnull().sum()/len(df))*100) # to check the missing values \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer  #handling the missing values using   Median for the income column \n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df['Income'] = imputer.fit_transform(df[['Income']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting year, month, and day  from Dt_Customer\n",
    "df['age']=2024 - df[\"Year_Birth\"] #create the age column \n",
    "\n",
    "\n",
    "df=df.drop(['Year_Birth'],axis=1)  # we drop the column \n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#univariate analysis \n",
    "numerical = df.select_dtypes(include=np.number)\n",
    "for numerical_column in list(numerical):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.displot(df[numerical_column], bins=20, color='blue', alpha=0.7)\n",
    "    plt.title(f'Distribution of {numerical_column}')\n",
    "    plt.xlabel(numerical_column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Z_CostContact',axis=1,inplace=True)\n",
    "df.drop('Z_Revenue',axis=1,inplace=True)  #low  variance in distribution \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for a categorical variable\n",
    "catergorical=df.select_dtypes(include=np.object)\n",
    "\n",
    "for catergorical_col in list(catergorical):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(df[catergorical_col], bins=20, color='blue', alpha=0.7)\n",
    "    plt.title(f'Distribution of {catergorical_col}')\n",
    "    plt.xlabel(catergorical_col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Marital_Status\"] = df[\"Marital_Status\"].replace({\"Married\": \"Married\", \"Together\": \"Married\", \"Absurd\": \"Single\",'YOLO':\"Single\",'Alone':\"Single\",'single':\"Single\"})\n",
    "print(set(df['Marital_Status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "#print(correlation_matrix)\n",
    "plt.figure(figsize=(20,20))  # Adjust the width and height as needed\n",
    "sns.heatmap(correlation_matrix, annot=True,  center = 0,cmap='viridis', fmt=\".2f\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "education_mapping = {        # there is a meaningful ranking \n",
    "    'Basic':1,\n",
    "    '2n Cycle':2,\n",
    "    'Graduation':3,\n",
    "    'Master':4,\n",
    "    'PhD':5\n",
    "}\n",
    "print(set(df['Education']))\n",
    "\n",
    "\n",
    "# Replace the categories with numerical values\n",
    "df['Education'] = df['Education'].map(education_mapping)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df['Marital_Status'])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "# Optionally, drop the original categorical column if desired\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Marital_Status',axis=1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    " \n",
    " # to find outliers \n",
    "numerical = df.select_dtypes(include=np.number)\n",
    "\n",
    "for numerical_column in list(numerical):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df[numerical_column], color='blue')\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'Boxplot of {numerical_column}')\n",
    "    plt.xlabel(numerical_column)\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_delete = df[df['age'] > 100].index\n",
    "\n",
    "df.drop(rows_to_delete, inplace=True)  # outliers handling delete the id with year of birth less than 1900\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    " \n",
    " # to find the distribtuion of the data  \n",
    "numerical = df.select_dtypes(include=np.number)\n",
    "\n",
    "for numerical_column in list(numerical):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Boxplot\n",
    "    sns.displot(data=df[numerical_column], color='blue')\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'distribution of {numerical_column}')\n",
    "    plt.xlabel(numerical_column)\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "data_to_transform = df[['Income']]  # Select the columns with high std \n",
    "scaler = MinMaxScaler()\n",
    "scaler_fit = scaler.fit(data_to_transform)\n",
    "normalized_data = scaler_fit.transform(data_to_transform)\n",
    "df[['Income']]=normalized_data\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use transformations when features have skewed distributions, \n",
    "and you want to make them more suitable for modeling.\n",
    "(Logarithmic Transformation):used to handle the right skewness\n",
    "\n",
    "df['MntWines'] = np.log1p(df['MntWines']) \n",
    "df['MntFruits'] = np.log1p(df['MntFruits'])\n",
    "df['MntMeatProducts'] = np.log1p(df['MntMeatProducts'])\n",
    "df['MntFishProducts'] = np.log1p(df['MntFishProducts'])\n",
    "df['MntSweetProducts'] = np.log1p(df['MntSweetProducts'])\n",
    "df['MntGoldProds'] = np.log1p(df['MntGoldProds'])\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of data\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "df_copy = df.copy()\n",
    "\n",
    "#cols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp2','AcceptedCmp5', 'AcceptedCmp1']\n",
    "#df_copy = df_copy.drop(cols_del, axis=1)\n",
    "# Scaling all the features inorder to apply the clustering algorithms \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_copy)\n",
    "scaled_ds = pd.DataFrame(scaler.transform(df_copy), columns=df_copy.columns)\n",
    "scaled_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionalty reduction to reduce the no of feature but capture the same amount of variance \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(scaled_ds)\n",
    "pca_ds = pd.DataFrame(pca.transform(scaled_ds), columns=([\"PC1\", \"PC2\",\"PC3\"]))\n",
    "pca_ds.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 3D Projection Of Data In The Reduced Dimension\n",
    "x = pca_ds[\"PC1\"]\n",
    "y = pca_ds[\"PC2\"]\n",
    "z = pca_ds[\"PC3\"]\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(x, y, z, c=\"blue\", marker=\"o\")\n",
    "ax.set_title(\"A 3D Projection Of Data In The Reduced Dimension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Quick examination of elbow method to find numbers of clusters to make.\n",
    "print('Elbow Method to determine the number of clusters to be formed:')\n",
    "visualizer = KElbowVisualizer(KMeans(), k=10,metric='distortion')\n",
    "visualizer.fit(pca_ds)\n",
    "visualizer.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the bend in the elbowcurve is at 4 clusters so n_clusters should be 4\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "# Fit the KMeans model to the data\n",
    "kmeans.fit(pca_ds)\n",
    "\n",
    "# Predict the cluster labels\n",
    "cluster_labels = kmeans.predict(pca_ds)\n",
    "# Assign cluster labels to the DataFrame\n",
    "df['kmeans_clusters'] = cluster_labels\n",
    "pca_ds[\"kmeans_clusters\"] = cluster_labels\n",
    "df1[\"kmeans_clusters\"] = cluster_labels\n",
    "silhouette = silhouette_score(pca_ds, cluster_labels)\n",
    "davies_bouldin = davies_bouldin_score(pca_ds, cluster_labels)\n",
    "calinski_harabasz = calinski_harabasz_score(pca_ds, cluster_labels)\n",
    "\n",
    "# Store important scores in a dictionary\n",
    "kmeans_scores = {\n",
    "    'Silhouette Score': silhouette,\n",
    "    'Davies-Bouldin Index': davies_bouldin,\n",
    "    'Calinski-Harabasz Index': calinski_harabasz\n",
    "}\n",
    "\n",
    "print(kmeans_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set axis \n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "x = pca_ds[\"PC1\"]\n",
    "y = pca_ds[\"PC2\"]\n",
    "z = pca_ds[\"PC3\"]\n",
    "\n",
    "\n",
    "\n",
    "#Plotting the clusters\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111, projection='3d', label=\"bla\")\n",
    "ax.scatter(x, y, z, s=40, c=pca_ds[\"kmeans_clusters\"], marker='o',cmap='viridis')\n",
    "ax.set_title(\"The Plot Of The kmeans_clusters\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#  PCA transformation with cluster labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Hierarchical Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hierarchical_model = AgglomerativeClustering(n_clusters=4)\n",
    "hierarchical_labels = hierarchical_model.fit_predict(pca_ds)\n",
    "df['Hierarchical_Clusters'] = hierarchical_labels\n",
    "pca_ds[\"Hierarchical_Clusters\"] = hierarchical_labels\n",
    "df1['Hierarchical_Clusters']=hierarchical_labels\n",
    "silhouette = silhouette_score(pca_ds, cluster_labels)\n",
    "davies_bouldin = davies_bouldin_score(pca_ds, cluster_labels)\n",
    "calinski_harabasz = calinski_harabasz_score(pca_ds, cluster_labels)\n",
    "\n",
    "# Store important scores in a dictionary\n",
    "Hierarchical_Clusters_scores = {\n",
    "    'Silhouette Score': silhouette,\n",
    "    'Davies-Bouldin Index': davies_bouldin,\n",
    "    'Calinski-Harabasz Index': calinski_harabasz\n",
    "}\n",
    "\n",
    "print(Hierarchical_Clusters_scores)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set axis \n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "x = pca_ds[\"PC1\"]\n",
    "y = pca_ds[\"PC2\"]\n",
    "z = pca_ds[\"PC3\"]\n",
    "\n",
    "\n",
    "\n",
    "#Plotting the clusters\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111, projection='3d', label=\"bla\")\n",
    "ax.scatter(x, y, z, s=40, c=pca_ds[\"Hierarchical_Clusters\"], marker='o',cmap='viridis')\n",
    "ax.set_title(\"The Plot Of The Hierarchical_Clusters\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans_scores)\n",
    "print(Hierarchical_Clusters_scores)\n",
    "# hierarchical_Clusters is better at clustering the points \n",
    "'''\n",
    "    For Silhouette Score and Calinski-Harabasz Index, higher values are preferred.\n",
    "    For Davies-Bouldin Index, lower values are preferred.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(13,8))\n",
    "pl = sns.countplot(x=df['Hierarchical_Clusters'])\n",
    "pl.set_title('Distribution Of The Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "df1['Spent'] = df1['MntWines'] + df1['MntFruits'] + df1['MntMeatProducts'] + df1['MntFishProducts'] + df1['MntSweetProducts'] + df1['MntGoldProds']\n",
    "pl = sns.scatterplot(data=df1, x=df1['Spent'], y=df1['Income'], hue=df1['Hierarchical_Clusters'],cmap='coolwarm')\n",
    "pl.set_title(\"Cluster's Profile Based on Income and Spending\")\n",
    "pl.set_xlabel('Total Spending')\n",
    "pl.set_ylabel('Income')\n",
    "\n",
    "# Adjust axis limits for better clarity\n",
    "plt.xlim(0, 2500)  # Set x-axis limits\n",
    "plt.ylim(0, 160000)  # Set y-axis limits\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# too much standardizaion have to see on non stadardized data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group 0: those with high  income and high spending \n",
    "Group 1: those with low  income and low spending \n",
    "Group 2: those with high income and moderate spending \n",
    "Group 3: those with moderate income and high spending\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Promos'] = df['AcceptedCmp1']+ df['AcceptedCmp2']+ df['AcceptedCmp3']+ df['AcceptedCmp4']+ df['AcceptedCmp5']\n",
    "plt.figure(figsize=(10,8))\n",
    "pl = sns.countplot(x=df['Total_Promos'], hue=df['Hierarchical_Clusters'])\n",
    "pl.set_title('Count Of Promotion Accepted')\n",
    "pl.set_xlabel('Number Of Total Accepted Promotions')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cluster 2 is mostly likely to accept promotions \n",
    "'''\n",
    "''' promotion 1 is the mostly successfully promotion which appeals to clusters of customers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop('Spent',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already selected the clusters\n",
    "selected_clusters= df1[df1['Hierarchical_Clusters'].isin([0])]\n",
    "\n",
    "\n",
    "# Selecting only numerical columns for distribution plot\n",
    "numerical_columns = selected_clusters.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Plotting distribution of numerical columns using displot with subplots\n",
    "plt.figure(figsize=(15, 5*selected_clusters.shape[1]))\n",
    "for i, column in enumerate(numerical_columns.columns):\n",
    "    plt.subplot(numerical_columns.shape[1], 2, i+1)\n",
    "    sns.histplot(selected_clusters[column], kde=True)\n",
    "    plt.title(f'Distribution of {column} in Cluster 0')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "selected_clusters.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already selected the clusters\n",
    "selected_clusters= df1[df1['Hierarchical_Clusters'].isin([1])]\n",
    "\n",
    "\n",
    "# Selecting only numerical columns for distribution plot\n",
    "numerical_columns = selected_clusters.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Plotting distribution of numerical columns using displot with subplots\n",
    "plt.figure(figsize=(15, 5*selected_clusters.shape[1]))\n",
    "for i, column in enumerate(numerical_columns.columns):\n",
    "    plt.subplot(numerical_columns.shape[1], 2, i+1)\n",
    "    sns.histplot(selected_clusters[column], kde=True)\n",
    "    plt.title(f'Distribution of {column} in Cluster 1')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "selected_clusters.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' in cluster0 \n",
    "customers mostly likely to have  graduation degrees\n",
    "income in the range of 31907-105471 with a mdeian income of 78423.500000\t\n",
    "they are unlikely to have kids or teenagers \n",
    "median age of 52 years and range of 29-81\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already selected the clusters\n",
    "selected_clusters = df1[df1['Hierarchical_Clusters'].isin([2])]\n",
    "\n",
    "# Selecting only numerical columns for distribution plot\n",
    "numerical_columns = selected_clusters.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Plotting distribution of numerical columns using displot with subplots\n",
    "plt.figure(figsize=(15, 5*selected_clusters.shape[1]))\n",
    "for i, column in enumerate(numerical_columns.columns):\n",
    "    plt.subplot(numerical_columns.shape[1], 2, i+1)\n",
    "    sns.histplot(selected_clusters[column], kde=True)\n",
    "    plt.title(f'Distribution of {column} in Cluster 2')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "selected_clusters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in cluster 1 \n",
    "The majority of individuals fall  are likely to have graduation \n",
    "median income is 34081 with a range 1730-162397\n",
    "they have mostly likely to have 1 kid \n",
    "median age of 51 and range of 28-84\n",
    "mostly likely to be married \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cluster2 \n",
    "they are most likely to have graduation or a phd \n",
    "they have a high median income of 81,128\n",
    "they have mostly likely to not have kids or teens \n",
    "they have a median age of 53 years of range of 29-81\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already selected the clusters\n",
    "selected_clusters = df1[df1['Hierarchical_Clusters'].isin([3])]\n",
    "\n",
    "# Selecting only numerical columns for distribution plot\n",
    "numerical_columns = selected_clusters.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Plotting distribution of numerical columns using displot with subplots\n",
    "plt.figure(figsize=(15, 5*selected_clusters.shape[1]))\n",
    "for i, column in enumerate(numerical_columns.columns):\n",
    "    plt.subplot(numerical_columns.shape[1], 2, i+1)\n",
    "    sns.histplot(selected_clusters[column], kde=True)\n",
    "    plt.title(f'Distribution of {column} in Cluster 3')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "selected_clusters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "# Save the pipeline to a file\n",
    "filename = 'customer_segment.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "\n",
    "# Load the model from disk\n",
    "with open(filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import streamlit as st\n",
    "\n",
    "# Streamlit setup\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = Pipeline([\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('pca', PCA(n_components=3))\n",
    "])\n",
    "\n",
    "# Define the clustering model\n",
    "clustering_model = AgglomerativeClustering()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('clustering', clustering_model)\n",
    "])\n",
    "\n",
    "@st.cache\n",
    "def load_data(file):\n",
    "    try:\n",
    "        data = pd.read_excel(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    st.title(\"Clustering Web App\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Upload Excel file\", type=[\"xlsx\"])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        data = load_data(uploaded_file)\n",
    "        if data is not None:\n",
    "            cluster_labels = pipeline.fit_predict(data)\n",
    "            results = {\n",
    "                'Silhouette Score': silhouette_score(data, cluster_labels),\n",
    "                'Davies-Bouldin Index': davies_bouldin_score(data, cluster_labels),\n",
    "                'Calinski-Harabasz Index': calinski_harabasz_score(data, cluster_labels),\n",
    "                'Cluster Labels': cluster_labels.tolist()\n",
    "            }\n",
    "\n",
    "            st.write(\"Clustering Results:\")\n",
    "            st.write(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 10:14:14.357 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Riddhima\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [13/Mar/2024 10:14:41] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [13/Mar/2024 10:14:41] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [13/Mar/2024 10:14:50] \"GET /predict HTTP/1.1\" 405 -\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import streamlit as st\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "# Streamlit setup\n",
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = Pipeline([\n",
    "    ('encoding', OneHotEncoder()),\n",
    "    ('standardize', StandardScaler(with_mean=False)),\n",
    "    ('svd', TruncatedSVD(n_components=3))  # Use TruncatedSVD instead of PCA\n",
    "])\n",
    "\n",
    "# Define the clustering model\n",
    "clustering_model = AgglomerativeClustering()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('clustering', clustering_model)\n",
    "])\n",
    "\n",
    "def load_data(file):\n",
    "    try:\n",
    "        data = pd.read_excel(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    st.title(\"Clustering Web App\")\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Upload Excel file\", type=[\"xlsx\"])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        data = load_data(uploaded_file)\n",
    "        if data is not None:\n",
    "            cluster_labels = pipeline.fit_predict(data)\n",
    "            results = {\n",
    "                'Silhouette Score': silhouette_score(data, cluster_labels),\n",
    "                'Davies-Bouldin Index': davies_bouldin_score(data, cluster_labels),\n",
    "                'Calinski-Harabasz Index': calinski_harabasz_score(data, cluster_labels),\n",
    "                'Cluster Labels': cluster_labels.tolist()\n",
    "            }\n",
    "\n",
    "            st.write(\"Clustering Results:\")\n",
    "            st.write(results)\n",
    "\n",
    "# Create a POST API\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file uploaded'}), 400\n",
    "    \n",
    "    file = request.files['file']\n",
    "    \n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'Empty file uploaded'}), 400\n",
    "    \n",
    "    if not file.filename.endswith('.xlsx'):\n",
    "        return jsonify({'error': 'Invalid file type. Only Excel files (.xlsx) are allowed.'}), 400\n",
    "    \n",
    "    data = load_data(file)\n",
    "    if data is None:\n",
    "        return jsonify({'error': 'An error occurred while reading the file.'}), 400\n",
    "    \n",
    "    cluster_labels = pipeline.fit_predict(data)\n",
    "    results = {\n",
    "        'Silhouette Score': silhouette_score(data, cluster_labels),\n",
    "        'Davies-Bouldin Index': davies_bouldin_score(data, cluster_labels),\n",
    "        'Calinski-Harabasz Index': calinski_harabasz_score(data, cluster_labels),\n",
    "        'Cluster Labels': cluster_labels.tolist()\n",
    "    }\n",
    "\n",
    "    return jsonify(results), 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    app.run(port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
